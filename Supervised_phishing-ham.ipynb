{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkQzXPCXUxme"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_spam_ham_phishing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    50199\n",
       "0    25220\n",
       "2     1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET ONLY PHISHING/HAM EMAILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1288, 95)\n"
     ]
    }
   ],
   "source": [
    "# Remove spam emails, only consider phishing: (ham = 0, spam = 1, phishing = 2)\n",
    "df_phish = df[(df['label'] == 2)]\n",
    "print(df_phish.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25220, 95)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ham data\n",
    "df_ham = df[(df['label'] == 0)]\n",
    "df_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13898, 95)\n"
     ]
    }
   ],
   "source": [
    "# Split ham data into 50/50 so some can be used for validation\n",
    "df_split = df_ham[:int(len(df_ham)/2)]\n",
    "df_split = df_split.reset_index()\n",
    "df_split = df_split.drop('index', axis=1)\n",
    "\n",
    "df_val_ham = df_ham[int(len(df_ham)/2):]\n",
    "df_val_ham = df_val_ham.reset_index()\n",
    "df_val_ham = df_val_ham.drop('index', axis=1)\n",
    "\n",
    "df = pd.concat([df_phish, df_split])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    12610\n",
       "2     1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>missing_subject</th>\n",
       "      <th>missing_to</th>\n",
       "      <th>missing_content-type</th>\n",
       "      <th>missing_mime-version</th>\n",
       "      <th>missing_x-mailer</th>\n",
       "      <th>missing_content-transfer-encoding</th>\n",
       "      <th>missing_x-mimeole</th>\n",
       "      <th>missing_x-priority</th>\n",
       "      <th>missing_list-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2576 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hops  missing_subject  missing_to  missing_content-type  \\\n",
       "0        1                0           0                     0   \n",
       "1        0                0           0                     0   \n",
       "2        2                0           0                     0   \n",
       "3        0                0           0                     0   \n",
       "4        0                0           0                     0   \n",
       "...    ...              ...         ...                   ...   \n",
       "2571     1                0           0                     0   \n",
       "2572     1                0           0                     0   \n",
       "2573     2                0           0                     0   \n",
       "2574     1                0           0                     0   \n",
       "2575     0                0           0                     0   \n",
       "\n",
       "      missing_mime-version  missing_x-mailer  \\\n",
       "0                        1                 1   \n",
       "1                        0                 1   \n",
       "2                        0                 1   \n",
       "3                        0                 1   \n",
       "4                        0                 1   \n",
       "...                    ...               ...   \n",
       "2571                     0                 0   \n",
       "2572                     0                 0   \n",
       "2573                     0                 1   \n",
       "2574                     0                 1   \n",
       "2575                     0                 1   \n",
       "\n",
       "      missing_content-transfer-encoding  missing_x-mimeole  \\\n",
       "0                                     0                  1   \n",
       "1                                     1                  1   \n",
       "2                                     0                  1   \n",
       "3                                     0                  1   \n",
       "4                                     1                  1   \n",
       "...                                 ...                ...   \n",
       "2571                                  0                  1   \n",
       "2572                                  1                  0   \n",
       "2573                                  0                  0   \n",
       "2574                                  1                  1   \n",
       "2575                                  1                  1   \n",
       "\n",
       "      missing_x-priority  missing_list-id  ...  \\\n",
       "0                      1                0  ...   \n",
       "1                      1                1  ...   \n",
       "2                      1                0  ...   \n",
       "3                      0                1  ...   \n",
       "4                      1                1  ...   \n",
       "...                  ...              ...  ...   \n",
       "2571                   1                0  ...   \n",
       "2572                   0                0  ...   \n",
       "2573                   1                0  ...   \n",
       "2574                   1                1  ...   \n",
       "2575                   0                1  ...   \n",
       "\n",
       "      domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                   1                         1   \n",
       "1                                   0                         0   \n",
       "2                                   0                         0   \n",
       "3                                   0                         0   \n",
       "4                                   0                         0   \n",
       "...                               ...                       ...   \n",
       "2571                                0                         0   \n",
       "2572                                1                         0   \n",
       "2573                                0                         0   \n",
       "2574                                0                         0   \n",
       "2575                                0                         0   \n",
       "\n",
       "      domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                    0                                    0   \n",
       "1                                    0                                    0   \n",
       "2                                    0                                    0   \n",
       "3                                    0                                    0   \n",
       "4                                    0                                    0   \n",
       "...                                ...                                  ...   \n",
       "2571                                 0                                    1   \n",
       "2572                                 0                                    0   \n",
       "2573                                 0                                    0   \n",
       "2574                                 0                                    0   \n",
       "2575                                 0                                    0   \n",
       "\n",
       "      domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                              0                           1   \n",
       "1                              0                           0   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           0   \n",
       "...                          ...                         ...   \n",
       "2571                           1                           0   \n",
       "2572                           0                           0   \n",
       "2573                           0                           0   \n",
       "2574                           0                           0   \n",
       "2575                           0                           0   \n",
       "\n",
       "      domain_match_to_from  domain_match_to_message-id  \\\n",
       "0                        1                           1   \n",
       "1                        1                           0   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "4                        0                           0   \n",
       "...                    ...                         ...   \n",
       "2571                     0                           0   \n",
       "2572                     0                           0   \n",
       "2573                     0                           0   \n",
       "2574                     1                           0   \n",
       "2575                     0                           0   \n",
       "\n",
       "      domain_match_to_received  label  \n",
       "0                            0      0  \n",
       "1                            0      1  \n",
       "2                            0      0  \n",
       "3                            0      1  \n",
       "4                            1      0  \n",
       "...                        ...    ...  \n",
       "2571                         0      0  \n",
       "2572                         0      0  \n",
       "2573                         0      0  \n",
       "2574                         0      1  \n",
       "2575                         0      1  \n",
       "\n",
       "[2576 rows x 95 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly Sample 1288 Ham emails to create a balanced dataset (match the number of phishing emails)\n",
    "df_ham = df[df['label'] == 0].sample(1288)\n",
    "df_phish = df[df['label'] == 2]\n",
    "\n",
    "df_phish = df_phish.assign(label=1)\n",
    "\n",
    "df_new = df_ham._append(df_phish, ignore_index=True)\n",
    "df_new = df_new.sample(frac=1)\n",
    "df = df_new.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1288\n",
       "1    1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce feature set\n",
    "# The only features that are kept are domain matching features, as these should generalize across very different email datasets without issue.\n",
    "\n",
    "feature_list = [\n",
    "'domain_match_from_return-path',\n",
    "'domain_match_message-id_from',\n",
    "'domain_match_message-id_return-path',\n",
    "'domain_match_to_from',\n",
    "'domain_match_errors-to_from',\n",
    "'domain_match_message-id_reply-to',\n",
    "'domain_match_errors-to_message-id',\n",
    "'domain_match_sender_from',\n",
    "'domain_match_to_received',\n",
    "'domain_match_errors-to_reply-to',\n",
    "'domain_match_to_message-id',\n",
    "'label']\n",
    "\n",
    "feature_list = ['domain_val_message-id',\n",
    "       'domain_match_message-id_from', 'domain_match_from_return-path',\n",
    "       'domain_match_message-id_return-path', 'domain_match_message-id_sender',\n",
    "       'domain_match_message-id_reply-to', 'domain_match_return-path_reply-to',\n",
    "       'domain_match_reply-to_to', 'domain_match_to_in-reply-to',\n",
    "       'domain_match_errors-to_message-id', 'domain_match_errors-to_from',\n",
    "       'domain_match_errors-to_sender', 'domain_match_errors-to_reply-to',\n",
    "       'domain_match_sender_from', 'domain_match_references_reply-to',\n",
    "       'domain_match_references_in-reply-to', 'domain_match_references_to',\n",
    "       'domain_match_from_reply-to', 'domain_match_to_from',\n",
    "       'domain_match_to_message-id', 'domain_match_to_received', 'label']\n",
    "\n",
    "df = df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df['label']\n",
    "df_X = df.drop('label', axis=1)\n",
    "\n",
    "features_list = df_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j6TAQHBjlhJ6"
   },
   "outputs": [],
   "source": [
    "# Apply a standard scaler to the full data set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df_X)\n",
    "df_X = scaler.transform(df_X)\n",
    "df_X = pd.DataFrame(df_X, columns=features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the data into a test and training set (20% test, 80% train)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2576, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_val_message-id</th>\n",
       "      <th>domain_match_message-id_from</th>\n",
       "      <th>domain_match_from_return-path</th>\n",
       "      <th>domain_match_message-id_return-path</th>\n",
       "      <th>domain_match_message-id_sender</th>\n",
       "      <th>domain_match_message-id_reply-to</th>\n",
       "      <th>domain_match_return-path_reply-to</th>\n",
       "      <th>domain_match_reply-to_to</th>\n",
       "      <th>domain_match_to_in-reply-to</th>\n",
       "      <th>domain_match_errors-to_message-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_val_message-id  domain_match_message-id_from  \\\n",
       "0                      0                             1   \n",
       "1                      0                             0   \n",
       "2                      0                             1   \n",
       "3                      0                             0   \n",
       "4                      0                             0   \n",
       "\n",
       "   domain_match_from_return-path  domain_match_message-id_return-path  \\\n",
       "0                              1                                    1   \n",
       "1                              1                                    0   \n",
       "2                              0                                    0   \n",
       "3                              0                                    0   \n",
       "4                              0                                    0   \n",
       "\n",
       "   domain_match_message-id_sender  domain_match_message-id_reply-to  \\\n",
       "0                               1                                 1   \n",
       "1                               0                                 0   \n",
       "2                               0                                 0   \n",
       "3                               0                                 0   \n",
       "4                               0                                 0   \n",
       "\n",
       "   domain_match_return-path_reply-to  domain_match_reply-to_to  \\\n",
       "0                                  1                         1   \n",
       "1                                  0                         0   \n",
       "2                                  0                         0   \n",
       "3                                  0                         0   \n",
       "4                                  1                         0   \n",
       "\n",
       "   domain_match_to_in-reply-to  domain_match_errors-to_message-id  ...  \\\n",
       "0                            0                                  1  ...   \n",
       "1                            0                                  0  ...   \n",
       "2                            0                                  0  ...   \n",
       "3                            0                                  0  ...   \n",
       "4                            0                                  0  ...   \n",
       "\n",
       "   domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                1                         1   \n",
       "1                                0                         0   \n",
       "2                                0                         0   \n",
       "3                                0                         0   \n",
       "4                                0                         0   \n",
       "\n",
       "   domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                 0                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                           0                           1   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   domain_match_to_from  domain_match_to_message-id  domain_match_to_received  \\\n",
       "0                     1                           1                         0   \n",
       "1                     1                           0                         0   \n",
       "2                     0                           0                         0   \n",
       "3                     0                           0                         0   \n",
       "4                     0                           0                         1   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOVELTY: GET VALIDATION DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed in order to have ham data\n",
    "df_val = pd.read_csv('preprocessed_phishing_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solution to get ham data is to transfer from other data...\n",
    "\n",
    "df_val = pd.concat([df_val, df_val_ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    12610\n",
       "2      245\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Sample 245 Ham emails to create a balanced dataset\n",
    "df_ham_val = df_val[df_val['label'] == 0].sample(245)\n",
    "\n",
    "df_phish_val = df_val[df_val['label'] == 2]\n",
    "\n",
    "df_phish_val = df_phish_val.assign(label=1)\n",
    "\n",
    "df_new_val = df_ham_val._append(df_phish_val, ignore_index=True)\n",
    "\n",
    "df_new_val = df_new_val.sample(frac=1)\n",
    "df_val = df_new_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce feature set\n",
    "# The only features that are kept are domain matching features, as these should generalize across very different email datasets without issue.\n",
    "\n",
    "feature_list = [\n",
    "'domain_match_from_return-path',\n",
    "'domain_match_message-id_from',\n",
    "'domain_match_message-id_return-path',\n",
    "'domain_match_to_from',\n",
    "'domain_match_errors-to_from',\n",
    "'domain_match_message-id_reply-to',\n",
    "'domain_match_errors-to_message-id',\n",
    "'domain_match_sender_from',\n",
    "'domain_match_to_received',\n",
    "'domain_match_errors-to_reply-to',\n",
    "'domain_match_to_message-id',\n",
    "'label']\n",
    "\n",
    "feature_list = ['domain_val_message-id',\n",
    "       'domain_match_message-id_from', 'domain_match_from_return-path',\n",
    "       'domain_match_message-id_return-path', 'domain_match_message-id_sender',\n",
    "       'domain_match_message-id_reply-to', 'domain_match_return-path_reply-to',\n",
    "       'domain_match_reply-to_to', 'domain_match_to_in-reply-to',\n",
    "       'domain_match_errors-to_message-id', 'domain_match_errors-to_from',\n",
    "       'domain_match_errors-to_sender', 'domain_match_errors-to_reply-to',\n",
    "       'domain_match_sender_from', 'domain_match_references_reply-to',\n",
    "       'domain_match_references_in-reply-to', 'domain_match_references_to',\n",
    "       'domain_match_from_reply-to', 'domain_match_to_from',\n",
    "       'domain_match_to_message-id', 'domain_match_to_received', 'label']\n",
    "\n",
    "df_val = df_val[feature_list]\n",
    "\n",
    "X_val = df_val.drop('label', axis=1)\n",
    "y_val = df_val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 22)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_val_message-id</th>\n",
       "      <th>domain_match_message-id_from</th>\n",
       "      <th>domain_match_from_return-path</th>\n",
       "      <th>domain_match_message-id_return-path</th>\n",
       "      <th>domain_match_message-id_sender</th>\n",
       "      <th>domain_match_message-id_reply-to</th>\n",
       "      <th>domain_match_return-path_reply-to</th>\n",
       "      <th>domain_match_reply-to_to</th>\n",
       "      <th>domain_match_to_in-reply-to</th>\n",
       "      <th>domain_match_errors-to_message-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_val_message-id  domain_match_message-id_from  \\\n",
       "0                      0                             0   \n",
       "1                      0                             1   \n",
       "2                      0                             1   \n",
       "3                      0                             1   \n",
       "4                      0                             1   \n",
       "\n",
       "   domain_match_from_return-path  domain_match_message-id_return-path  \\\n",
       "0                              0                                    1   \n",
       "1                              0                                    0   \n",
       "2                              0                                    0   \n",
       "3                              1                                    1   \n",
       "4                              0                                    0   \n",
       "\n",
       "   domain_match_message-id_sender  domain_match_message-id_reply-to  \\\n",
       "0                               0                                 0   \n",
       "1                               0                                 0   \n",
       "2                               0                                 0   \n",
       "3                               0                                 0   \n",
       "4                               0                                 0   \n",
       "\n",
       "   domain_match_return-path_reply-to  domain_match_reply-to_to  \\\n",
       "0                                  0                         0   \n",
       "1                                  0                         0   \n",
       "2                                  0                         0   \n",
       "3                                  0                         0   \n",
       "4                                  0                         0   \n",
       "\n",
       "   domain_match_to_in-reply-to  domain_match_errors-to_message-id  ...  \\\n",
       "0                            0                                  0  ...   \n",
       "1                            1                                  0  ...   \n",
       "2                            0                                  0  ...   \n",
       "3                            0                                  0  ...   \n",
       "4                            0                                  0  ...   \n",
       "\n",
       "   domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                0                         0   \n",
       "1                                0                         0   \n",
       "2                                0                         0   \n",
       "3                                0                         0   \n",
       "4                                0                         0   \n",
       "\n",
       "   domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                 0                                    0   \n",
       "1                                 0                                    1   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                           0                           0   \n",
       "1                           1                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           1                           0   \n",
       "\n",
       "   domain_match_to_from  domain_match_to_message-id  domain_match_to_received  \\\n",
       "0                     0                           0                         0   \n",
       "1                     1                           1                         0   \n",
       "2                     0                           0                         0   \n",
       "3                     0                           0                         0   \n",
       "4                     0                           0                         0   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XR4Wc3Fqh_W"
   },
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__criterion': 'entropy', 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 3, 'rf__n_estimators': 150} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('rf',\n",
      "                 RandomForestClassifier(criterion='entropy', min_samples_leaf=2,\n",
      "                                        min_samples_split=3,\n",
      "                                        n_estimators=150))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 76.53061224489795\n",
      "F1 Score: 80.99173553719008\n",
      "Recall: 100.0\n",
      "Precision: 68.05555555555556\n",
      "ROC AUC: 76.53061224489795\n",
      "Confusion Matrix: [[130 115]\n",
      " [  0 245]]\n",
      "CPU times: user 645 ms, sys: 164 ms, total: 809 ms\n",
      "Wall time: 5.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"rf\", RandomForestClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'rf__n_estimators': [100, 150],\n",
    "                  'rf__criterion': ['entropy', 'gini'],\n",
    "                  'rf__min_samples_split': [2, 3],\n",
    "                  'rf__min_samples_leaf': [1, 2],\n",
    "                  'rf__max_features': ['sqrt', 'log2']} # removed 'auto' \n",
    "         \n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "rf_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "original_predict = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, original_predict)*100)\n",
    "print('F1 Score:', f1_score(y_test, original_predict)*100)\n",
    "print('Recall:', recall_score(y_test, original_predict)*100)\n",
    "print('Precision:', precision_score(y_test, original_predict)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, original_predict)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, original_predict))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty: Trying hyperparameter tuning with RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliechow/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 32 is smaller than n_iter=500. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'rf__n_estimators': 100, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 2, 'rf__max_features': 'sqrt', 'rf__criterion': 'gini'}\n",
      "Best score is: 0.9810679611650486\n",
      "Best model is: Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('rf', RandomForestClassifier(min_samples_leaf=2))])\n",
      "\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 76.53061224489795\n",
      "F1 Score: 80.99173553719008\n",
      "Recall: 100.0\n",
      "Precision: 68.05555555555556\n",
      "ROC AUC: 76.53061224489795\n",
      "Confusion Matrix: [[130 115]\n",
      " [  0 245]]\n",
      "CPU times: user 380 ms, sys: 61.5 ms, total: 442 ms\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter Tuning with RANDOM SEARCH\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"rf\", RandomForestClassifier())\n",
    "                ])\n",
    "\n",
    "rs_space={'rf__n_estimators': [100, 150],\n",
    "               'rf__criterion': ['entropy', 'gini'],\n",
    "               'rf__min_samples_split': [2, 3],\n",
    "               'rf__min_samples_leaf': [1, 2],\n",
    "               'rf__max_features': ['sqrt', 'log2']\n",
    "         }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_random = RandomizedSearchCV(pipe, rs_space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=10)\n",
    "model_random = rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_df = pd.DataFrame(model_random.cv_results_)\n",
    "print('Best hyperparameters are: '+str(model_random.best_params_))\n",
    "print('Best score is: '+str(model_random.best_score_))\n",
    "print('Best model is: '+str(model_random.best_estimator_))\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = model_random.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "original_predict = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('\\nORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, original_predict)*100)\n",
    "print('F1 Score:', f1_score(y_test, original_predict)*100)\n",
    "print('Recall:', recall_score(y_test, original_predict)*100)\n",
    "print('Precision:', precision_score(y_test, original_predict)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, original_predict)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, original_predict))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#rf_df[rf_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBX1OIt-s3J8"
   },
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp__activation': 'relu', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (40, 40), 'mlp__learning_rate': 'constant', 'mlp__solver': 'sgd'} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('mlp',\n",
      "                 MLPClassifier(hidden_layer_sizes=(40, 40), max_iter=500,\n",
      "                               solver='sgd'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.48062015503875\n",
      "F1 Score: 97.56097560975608\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.58823529411765\n",
      "ROC AUC: 97.45548794230334\n",
      "Confusion Matrix: [[243  12]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 91.63265306122449\n",
      "F1 Score: 91.91321499013806\n",
      "Recall: 95.10204081632652\n",
      "Precision: 88.93129770992367\n",
      "ROC AUC: 91.63265306122447\n",
      "Confusion Matrix: [[216  29]\n",
      " [ 12 233]]\n",
      "CPU times: user 24.5 s, sys: 1.53 s, total: 26.1 s\n",
      "Wall time: 40.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"mlp\", MLPClassifier(max_iter=500))\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'mlp__hidden_layer_sizes': [(20,), (20,20), (40,), (40,40)],\n",
    "                   'mlp__activation': ['tanh', 'relu'],\n",
    "                   'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "                   'mlp__solver': ['adam', 'sgd'],\n",
    "                   'mlp__alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "mlp_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#mlp_df[mlp_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty: Trying hyperparameter tuning with RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'mlp__solver': 'adam', 'mlp__learning_rate': 'constant', 'mlp__hidden_layer_sizes': (20,), 'mlp__alpha': 0.0001, 'mlp__activation': 'tanh'}\n",
      "Best score is: 0.9805820435889058\n",
      "Best model is: Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('mlp',\n",
      "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(20,),\n",
      "                               max_iter=500))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 91.42857142857143\n",
      "F1 Score: 90.78947368421053\n",
      "Recall: 84.48979591836735\n",
      "Precision: 98.10426540284361\n",
      "ROC AUC: 91.42857142857143\n",
      "Confusion Matrix: [[241   4]\n",
      " [ 38 207]]\n",
      "CPU times: user 769 ms, sys: 151 ms, total: 921 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter Tuning with RANDOM SEARCH\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"mlp\", MLPClassifier(max_iter=500))\n",
    "                ])\n",
    "\n",
    "rs_space={'mlp__hidden_layer_sizes': [(20,), (20,20), (40,), (40,40)],\n",
    "            'mlp__activation': ['tanh', 'relu'],\n",
    "            'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "            'mlp__solver': ['adam', 'sgd'],\n",
    "            'mlp__alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "mlp_random = RandomizedSearchCV(pipe, rs_space, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "model_random = mlp_random.fit(X_train, y_train)\n",
    "\n",
    "mlp_df = pd.DataFrame(model_random.cv_results_)\n",
    "print('Best hyperparameters are: '+str(model_random.best_params_))\n",
    "print('Best score is: '+str(model_random.best_score_))\n",
    "print('Best model is: '+str(model_random.best_estimator_))\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = model_random.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#mlp_df[mlp_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 10, 'lr__fit_intercept': True, 'lr__penalty': 'l2', 'lr__solver': 'newton-cg', 'lr__tol': 0.0001} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('lr',\n",
      "                 LogisticRegression(C=10, max_iter=1000, solver='newton-cg'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 80.81632653061224\n",
      "F1 Score: 76.73267326732675\n",
      "Recall: 63.26530612244898\n",
      "Precision: 97.48427672955975\n",
      "ROC AUC: 80.81632653061224\n",
      "Confusion Matrix: [[241   4]\n",
      " [ 90 155]]\n",
      "CPU times: user 482 ms, sys: 63.1 ms, total: 545 ms\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"lr\", LogisticRegression(max_iter=1000))\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'lr__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                  'lr__fit_intercept': [True, False],\n",
    "                  'lr__tol': [0.0001, 0.001],\n",
    "                  'lr__penalty': ['l2'],\n",
    "                  'lr__C': [0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "lr_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#lr_df[lr_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 1, 'svc__degree': 3, 'svc__kernel': 'rbf', 'svc__tol': 0.001} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()), ('svc', SVC(C=1))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 80.61224489795919\n",
      "F1 Score: 83.06595365418895\n",
      "Recall: 95.10204081632652\n",
      "Precision: 73.73417721518987\n",
      "ROC AUC: 80.61224489795917\n",
      "Confusion Matrix: [[162  83]\n",
      " [ 12 233]]\n",
      "CPU times: user 1.27 s, sys: 485 ms, total: 1.75 s\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"svc\", SVC())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'svc__C': [0.1, 1, 10],\n",
    "                  'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  'svc__degree': [3, 4, 5],\n",
    "                  'svc__tol': [0.001, 0.0001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "svm_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#svm_df[svm_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__ccp_alpha': 0, 'dt__criterion': 'entropy', 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 2} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('dt',\n",
      "                 DecisionTreeClassifier(ccp_alpha=0, criterion='entropy'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 76.53061224489795\n",
      "F1 Score: 80.99173553719008\n",
      "Recall: 100.0\n",
      "Precision: 68.05555555555556\n",
      "ROC AUC: 76.53061224489795\n",
      "Confusion Matrix: [[130 115]\n",
      " [  0 245]]\n",
      "CPU times: user 319 ms, sys: 66.1 ms, total: 385 ms\n",
      "Wall time: 550 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"dt\", DecisionTreeClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'dt__criterion': ['entropy', 'gini'],\n",
    "                  'dt__min_samples_split': [2, 3, 4],\n",
    "                  'dt__min_samples_leaf': [1, 2, 3],\n",
    "                  'dt__ccp_alpha': [0, 0.005, 0.01, 0.025, 0.05, 0.1]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "dt_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#dt_df[dt_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes (Gaussian)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gnb__var_smoothing': 1e-08} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('gnb', GaussianNB(var_smoothing=1e-08))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 89.72868217054264\n",
      "F1 Score: 90.68541300527241\n",
      "Recall: 98.85057471264368\n",
      "Precision: 83.76623376623377\n",
      "ROC AUC: 89.62136578769439\n",
      "Confusion Matrix: [[205  50]\n",
      " [  3 258]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 50.0\n",
      "F1 Score: 0.0\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "ROC AUC: 50.0\n",
      "Confusion Matrix: [[245   0]\n",
      " [245   0]]\n",
      "CPU times: user 31.2 ms, sys: 8.53 ms, total: 39.7 ms\n",
      "Wall time: 55.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliechow/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"gnb\", GaussianNB())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'gnb__var_smoothing': [1E-9, 1E-10, 1E-8]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "nb_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#nb_df[nb_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab__algorithm': 'SAMME.R', 'ab__learning_rate': 0.95, 'ab__n_estimators': 100} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('ab',\n",
      "                 AdaBoostClassifier(learning_rate=0.95, n_estimators=100))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.28682170542635\n",
      "F1 Score: 97.35849056603773\n",
      "Recall: 98.85057471264368\n",
      "Precision: 95.91078066914498\n",
      "ROC AUC: 97.26842461122381\n",
      "Confusion Matrix: [[244  11]\n",
      " [  3 258]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 81.83673469387756\n",
      "F1 Score: 84.6286701208981\n",
      "Recall: 100.0\n",
      "Precision: 73.35329341317365\n",
      "ROC AUC: 81.83673469387756\n",
      "Confusion Matrix: [[156  89]\n",
      " [  0 245]]\n",
      "CPU times: user 676 ms, sys: 110 ms, total: 786 ms\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"ab\", AdaBoostClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'ab__n_estimators': [50, 100, 150, 200],\n",
    "                  'ab__learning_rate': [0.95, 1, 1.05, 1.25, 1.5, 1.75, 2],\n",
    "                  'ab__algorithm': ['SAMME', 'SAMME.R']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "ab_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#ab_df[ab_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GradientBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__learning_rate': 0.1, 'gbc__max_features': 'log2', 'gbc__n_estimators': 100} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('gbc', GradientBoostingClassifier(max_features='log2'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 76.53061224489795\n",
      "F1 Score: 80.99173553719008\n",
      "Recall: 100.0\n",
      "Precision: 68.05555555555556\n",
      "ROC AUC: 76.53061224489795\n",
      "Confusion Matrix: [[130 115]\n",
      " [  0 245]]\n",
      "CPU times: user 441 ms, sys: 60.2 ms, total: 501 ms\n",
      "Wall time: 3.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"gbc\", GradientBoostingClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'gbc__max_features': ['sqrt', 'log2'], # Removed 'auto'\n",
    "                   'gbc__learning_rate': [0.05, 0.1, 0.2, 0.25, 0.30, 0.35, 0.40, 0.5, 0.6, 0.7, 0.9],\n",
    "                   'gbc__n_estimators': [100, 200]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "gb_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#gb_df[gb_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__algorithm': 'auto', 'knn__leaf_size': 15, 'knn__n_neighbors': 20, 'knn__p': 2, 'knn__weights': 'distance'} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(leaf_size=15, n_neighbors=20,\n",
      "                                      weights='distance'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 97.67441860465115\n",
      "F1 Score: 97.74436090225565\n",
      "Recall: 99.61685823754789\n",
      "Precision: 95.9409594095941\n",
      "ROC AUC: 97.65156637367592\n",
      "Confusion Matrix: [[244  11]\n",
      " [  1 260]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 75.71428571428571\n",
      "F1 Score: 79.65811965811966\n",
      "Recall: 95.10204081632652\n",
      "Precision: 68.52941176470588\n",
      "ROC AUC: 75.71428571428571\n",
      "Confusion Matrix: [[138 107]\n",
      " [ 12 233]]\n",
      "CPU times: user 2.73 s, sys: 403 ms, total: 3.14 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"knn\", KNeighborsClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'knn__n_neighbors': [1, 10, 20],\n",
    "                  'knn__weights': ['uniform', 'distance'],\n",
    "                  'knn__p': [1, 2],\n",
    "                  'knn__algorithm': ['auto'],\n",
    "                  'knn__leaf_size': [15, 30, 45]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "knn_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#knn_df[knn_df['rank_test_score'] <= 5].head(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sCMEqns8jY-1",
    "F5A997Ekjh0E",
    "KvWjb8GwOM-y",
    "rJJ4WHSBFTWc",
    "nqHSyA4SyJ7K",
    "xhkNe2ql-cFP",
    "fMwsPWxpi2cG",
    "-_nEjwFOjUDI",
    "MyekmTK3q5TC",
    "RkQzXPCXUxme",
    "P1zXPP6trxSN",
    "iGZ561te2ONO",
    "rffvHbuzGO8c"
   ],
   "name": "Capstone Project - Email Headers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
