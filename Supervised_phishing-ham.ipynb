{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkQzXPCXUxme"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_spam_ham_phishing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    50199\n",
       "0    25220\n",
       "2     1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET ONLY PHISHING/HAM EMAILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1288, 95)\n"
     ]
    }
   ],
   "source": [
    "# Remove spam emails, only consider phishing: (ham = 0, spam = 1, phishing = 2)\n",
    "df_phish = df[(df['label'] == 2)]\n",
    "print(df_phish.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25220, 95)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ham data\n",
    "df_ham = df[(df['label'] == 0)]\n",
    "df_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13898, 95)\n"
     ]
    }
   ],
   "source": [
    "# Split ham data into 50/50 so some can be used for validation\n",
    "df_split = df_ham[:int(len(df_ham)/2)]\n",
    "df_split = df_split.reset_index()\n",
    "df_split = df_split.drop('index', axis=1)\n",
    "\n",
    "df_val_ham = df_ham[int(len(df_ham)/2):]\n",
    "df_val_ham = df_val_ham.reset_index()\n",
    "df_val_ham = df_val_ham.drop('index', axis=1)\n",
    "\n",
    "df = pd.concat([df_phish, df_split])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    12610\n",
       "2     1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>missing_subject</th>\n",
       "      <th>missing_to</th>\n",
       "      <th>missing_content-type</th>\n",
       "      <th>missing_mime-version</th>\n",
       "      <th>missing_x-mailer</th>\n",
       "      <th>missing_content-transfer-encoding</th>\n",
       "      <th>missing_x-mimeole</th>\n",
       "      <th>missing_x-priority</th>\n",
       "      <th>missing_list-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2576 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hops  missing_subject  missing_to  missing_content-type  \\\n",
       "0        0                0           0                     0   \n",
       "1        1                0           0                     0   \n",
       "2        0                0           1                     0   \n",
       "3        1                0           0                     0   \n",
       "4        1                0           0                     0   \n",
       "...    ...              ...         ...                   ...   \n",
       "2571     1                0           1                     0   \n",
       "2572     2                0           0                     0   \n",
       "2573     1                0           0                     0   \n",
       "2574     1                0           0                     0   \n",
       "2575     1                0           0                     0   \n",
       "\n",
       "      missing_mime-version  missing_x-mailer  \\\n",
       "0                        0                 1   \n",
       "1                        0                 1   \n",
       "2                        0                 1   \n",
       "3                        0                 1   \n",
       "4                        0                 1   \n",
       "...                    ...               ...   \n",
       "2571                     0                 1   \n",
       "2572                     0                 1   \n",
       "2573                     0                 1   \n",
       "2574                     0                 1   \n",
       "2575                     0                 1   \n",
       "\n",
       "      missing_content-transfer-encoding  missing_x-mimeole  \\\n",
       "0                                     1                  1   \n",
       "1                                     1                  1   \n",
       "2                                     1                  1   \n",
       "3                                     1                  1   \n",
       "4                                     0                  1   \n",
       "...                                 ...                ...   \n",
       "2571                                  1                  1   \n",
       "2572                                  0                  1   \n",
       "2573                                  1                  1   \n",
       "2574                                  1                  1   \n",
       "2575                                  0                  1   \n",
       "\n",
       "      missing_x-priority  missing_list-id  ...  \\\n",
       "0                      1                1  ...   \n",
       "1                      1                1  ...   \n",
       "2                      1                1  ...   \n",
       "3                      1                1  ...   \n",
       "4                      1                0  ...   \n",
       "...                  ...              ...  ...   \n",
       "2571                   1                1  ...   \n",
       "2572                   1                0  ...   \n",
       "2573                   1                1  ...   \n",
       "2574                   1                1  ...   \n",
       "2575                   1                0  ...   \n",
       "\n",
       "      domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                   0                         0   \n",
       "1                                   0                         0   \n",
       "2                                   0                         0   \n",
       "3                                   0                         0   \n",
       "4                                   0                         1   \n",
       "...                               ...                       ...   \n",
       "2571                                0                         0   \n",
       "2572                                0                         0   \n",
       "2573                                0                         0   \n",
       "2574                                0                         0   \n",
       "2575                                0                         0   \n",
       "\n",
       "      domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                    0                                    0   \n",
       "1                                    0                                    1   \n",
       "2                                    0                                    0   \n",
       "3                                    0                                    0   \n",
       "4                                    0                                    1   \n",
       "...                                ...                                  ...   \n",
       "2571                                 0                                    0   \n",
       "2572                                 0                                    0   \n",
       "2573                                 0                                    1   \n",
       "2574                                 0                                    0   \n",
       "2575                                 0                                    0   \n",
       "\n",
       "      domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                              0                           0   \n",
       "1                              1                           0   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              1                           0   \n",
       "...                          ...                         ...   \n",
       "2571                           0                           0   \n",
       "2572                           0                           0   \n",
       "2573                           0                           0   \n",
       "2574                           0                           0   \n",
       "2575                           0                           0   \n",
       "\n",
       "      domain_match_to_from  domain_match_to_message-id  \\\n",
       "0                        0                           0   \n",
       "1                        1                           1   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "4                        0                           0   \n",
       "...                    ...                         ...   \n",
       "2571                     0                           0   \n",
       "2572                     0                           0   \n",
       "2573                     1                           0   \n",
       "2574                     0                           0   \n",
       "2575                     0                           0   \n",
       "\n",
       "      domain_match_to_received  label  \n",
       "0                            0      1  \n",
       "1                            0      1  \n",
       "2                            0      1  \n",
       "3                            0      1  \n",
       "4                            0      0  \n",
       "...                        ...    ...  \n",
       "2571                         0      1  \n",
       "2572                         0      0  \n",
       "2573                         0      1  \n",
       "2574                         0      1  \n",
       "2575                         0      0  \n",
       "\n",
       "[2576 rows x 95 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly Sample 1288 Ham emails to create a balanced dataset (match the number of phishing emails)\n",
    "df_ham = df[df['label'] == 0].sample(1288)\n",
    "df_phish = df[df['label'] == 2]\n",
    "\n",
    "df_phish = df_phish.assign(label=1)\n",
    "\n",
    "df_new = df_ham._append(df_phish, ignore_index=True)\n",
    "df_new = df_new.sample(frac=1)\n",
    "df = df_new.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1288\n",
       "0    1288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce feature set\n",
    "# The only features that are kept are domain matching features, as these should generalize across very different email datasets without issue.\n",
    "\n",
    "feature_list = [\n",
    "'domain_match_from_return-path',\n",
    "'domain_match_message-id_from',\n",
    "'domain_match_message-id_return-path',\n",
    "'domain_match_to_from',\n",
    "'domain_match_errors-to_from',\n",
    "'domain_match_message-id_reply-to',\n",
    "'domain_match_errors-to_message-id',\n",
    "'domain_match_sender_from',\n",
    "'domain_match_to_received',\n",
    "'domain_match_errors-to_reply-to',\n",
    "'domain_match_to_message-id',\n",
    "'label']\n",
    "\n",
    "feature_list = ['domain_val_message-id',\n",
    "       'domain_match_message-id_from', 'domain_match_from_return-path',\n",
    "       'domain_match_message-id_return-path', 'domain_match_message-id_sender',\n",
    "       'domain_match_message-id_reply-to', 'domain_match_return-path_reply-to',\n",
    "       'domain_match_reply-to_to', 'domain_match_to_in-reply-to',\n",
    "       'domain_match_errors-to_message-id', 'domain_match_errors-to_from',\n",
    "       'domain_match_errors-to_sender', 'domain_match_errors-to_reply-to',\n",
    "       'domain_match_sender_from', 'domain_match_references_reply-to',\n",
    "       'domain_match_references_in-reply-to', 'domain_match_references_to',\n",
    "       'domain_match_from_reply-to', 'domain_match_to_from',\n",
    "       'domain_match_to_message-id', 'domain_match_to_received', 'label']\n",
    "\n",
    "df = df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df['label']\n",
    "df_X = df.drop('label', axis=1)\n",
    "\n",
    "features_list = df_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "j6TAQHBjlhJ6"
   },
   "outputs": [],
   "source": [
    "# Apply a standard scaler to the full data set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df_X)\n",
    "df_X = scaler.transform(df_X)\n",
    "df_X = pd.DataFrame(df_X, columns=features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the data into a test and training set (20% test, 80% train)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2576, 22)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_val_message-id</th>\n",
       "      <th>domain_match_message-id_from</th>\n",
       "      <th>domain_match_from_return-path</th>\n",
       "      <th>domain_match_message-id_return-path</th>\n",
       "      <th>domain_match_message-id_sender</th>\n",
       "      <th>domain_match_message-id_reply-to</th>\n",
       "      <th>domain_match_return-path_reply-to</th>\n",
       "      <th>domain_match_reply-to_to</th>\n",
       "      <th>domain_match_to_in-reply-to</th>\n",
       "      <th>domain_match_errors-to_message-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_val_message-id  domain_match_message-id_from  \\\n",
       "0                      0                             0   \n",
       "1                      0                             1   \n",
       "2                      0                             0   \n",
       "3                      0                             0   \n",
       "4                      0                             1   \n",
       "\n",
       "   domain_match_from_return-path  domain_match_message-id_return-path  \\\n",
       "0                              0                                    0   \n",
       "1                              1                                    1   \n",
       "2                              1                                    0   \n",
       "3                              1                                    0   \n",
       "4                              1                                    1   \n",
       "\n",
       "   domain_match_message-id_sender  domain_match_message-id_reply-to  \\\n",
       "0                               0                                 0   \n",
       "1                               0                                 0   \n",
       "2                               0                                 0   \n",
       "3                               0                                 0   \n",
       "4                               1                                 0   \n",
       "\n",
       "   domain_match_return-path_reply-to  domain_match_reply-to_to  \\\n",
       "0                                  0                         0   \n",
       "1                                  0                         0   \n",
       "2                                  0                         0   \n",
       "3                                  0                         0   \n",
       "4                                  0                         0   \n",
       "\n",
       "   domain_match_to_in-reply-to  domain_match_errors-to_message-id  ...  \\\n",
       "0                            0                                  0  ...   \n",
       "1                            1                                  0  ...   \n",
       "2                            0                                  0  ...   \n",
       "3                            0                                  0  ...   \n",
       "4                            1                                  1  ...   \n",
       "\n",
       "   domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                0                         0   \n",
       "1                                0                         0   \n",
       "2                                0                         0   \n",
       "3                                0                         0   \n",
       "4                                0                         1   \n",
       "\n",
       "   domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                 0                                    0   \n",
       "1                                 0                                    1   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    1   \n",
       "\n",
       "   domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                           0                           0   \n",
       "1                           1                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           1                           0   \n",
       "\n",
       "   domain_match_to_from  domain_match_to_message-id  domain_match_to_received  \\\n",
       "0                     0                           0                         0   \n",
       "1                     1                           1                         0   \n",
       "2                     0                           0                         0   \n",
       "3                     0                           0                         0   \n",
       "4                     0                           0                         0   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOVELTY: GET VALIDATION DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed in order to have ham data\n",
    "df_val = pd.read_csv('preprocessed_phishing_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solution to get ham data is to transfer from other data...\n",
    "\n",
    "df_val = pd.concat([df_val, df_val_ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    12610\n",
       "2      245\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Sample 245 Ham emails to create a balanced dataset\n",
    "df_ham_val = df_val[df_val['label'] == 0].sample(245)\n",
    "\n",
    "df_phish_val = df_val[df_val['label'] == 2]\n",
    "\n",
    "df_phish_val = df_phish_val.assign(label=1)\n",
    "\n",
    "df_new_val = df_ham_val._append(df_phish_val, ignore_index=True)\n",
    "\n",
    "df_new_val = df_new_val.sample(frac=1)\n",
    "df_val = df_new_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce feature set\n",
    "# The only features that are kept are domain matching features, as these should generalize across very different email datasets without issue.\n",
    "\n",
    "feature_list = [\n",
    "'domain_match_from_return-path',\n",
    "'domain_match_message-id_from',\n",
    "'domain_match_message-id_return-path',\n",
    "'domain_match_to_from',\n",
    "'domain_match_errors-to_from',\n",
    "'domain_match_message-id_reply-to',\n",
    "'domain_match_errors-to_message-id',\n",
    "'domain_match_sender_from',\n",
    "'domain_match_to_received',\n",
    "'domain_match_errors-to_reply-to',\n",
    "'domain_match_to_message-id',\n",
    "'label']\n",
    "\n",
    "feature_list = ['domain_val_message-id',\n",
    "       'domain_match_message-id_from', 'domain_match_from_return-path',\n",
    "       'domain_match_message-id_return-path', 'domain_match_message-id_sender',\n",
    "       'domain_match_message-id_reply-to', 'domain_match_return-path_reply-to',\n",
    "       'domain_match_reply-to_to', 'domain_match_to_in-reply-to',\n",
    "       'domain_match_errors-to_message-id', 'domain_match_errors-to_from',\n",
    "       'domain_match_errors-to_sender', 'domain_match_errors-to_reply-to',\n",
    "       'domain_match_sender_from', 'domain_match_references_reply-to',\n",
    "       'domain_match_references_in-reply-to', 'domain_match_references_to',\n",
    "       'domain_match_from_reply-to', 'domain_match_to_from',\n",
    "       'domain_match_to_message-id', 'domain_match_to_received', 'label']\n",
    "\n",
    "df_val = df_val[feature_list]\n",
    "\n",
    "X_val = df_val.drop('label', axis=1)\n",
    "y_val = df_val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 22)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_val_message-id</th>\n",
       "      <th>domain_match_message-id_from</th>\n",
       "      <th>domain_match_from_return-path</th>\n",
       "      <th>domain_match_message-id_return-path</th>\n",
       "      <th>domain_match_message-id_sender</th>\n",
       "      <th>domain_match_message-id_reply-to</th>\n",
       "      <th>domain_match_return-path_reply-to</th>\n",
       "      <th>domain_match_reply-to_to</th>\n",
       "      <th>domain_match_to_in-reply-to</th>\n",
       "      <th>domain_match_errors-to_message-id</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_match_errors-to_reply-to</th>\n",
       "      <th>domain_match_sender_from</th>\n",
       "      <th>domain_match_references_reply-to</th>\n",
       "      <th>domain_match_references_in-reply-to</th>\n",
       "      <th>domain_match_references_to</th>\n",
       "      <th>domain_match_from_reply-to</th>\n",
       "      <th>domain_match_to_from</th>\n",
       "      <th>domain_match_to_message-id</th>\n",
       "      <th>domain_match_to_received</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_val_message-id  domain_match_message-id_from  \\\n",
       "0                      0                             0   \n",
       "1                      0                             1   \n",
       "2                      0                             1   \n",
       "3                      0                             1   \n",
       "4                      0                             0   \n",
       "\n",
       "   domain_match_from_return-path  domain_match_message-id_return-path  \\\n",
       "0                              0                                    0   \n",
       "1                              1                                    1   \n",
       "2                              1                                    1   \n",
       "3                              0                                    0   \n",
       "4                              0                                    0   \n",
       "\n",
       "   domain_match_message-id_sender  domain_match_message-id_reply-to  \\\n",
       "0                               0                                 0   \n",
       "1                               0                                 0   \n",
       "2                               0                                 1   \n",
       "3                               0                                 0   \n",
       "4                               0                                 0   \n",
       "\n",
       "   domain_match_return-path_reply-to  domain_match_reply-to_to  \\\n",
       "0                                  0                         0   \n",
       "1                                  0                         1   \n",
       "2                                  1                         0   \n",
       "3                                  0                         0   \n",
       "4                                  0                         0   \n",
       "\n",
       "   domain_match_to_in-reply-to  domain_match_errors-to_message-id  ...  \\\n",
       "0                            1                                  0  ...   \n",
       "1                            0                                  0  ...   \n",
       "2                            0                                  0  ...   \n",
       "3                            0                                  0  ...   \n",
       "4                            1                                  0  ...   \n",
       "\n",
       "   domain_match_errors-to_reply-to  domain_match_sender_from  \\\n",
       "0                                0                         0   \n",
       "1                                0                         0   \n",
       "2                                0                         0   \n",
       "3                                0                         0   \n",
       "4                                0                         0   \n",
       "\n",
       "   domain_match_references_reply-to  domain_match_references_in-reply-to  \\\n",
       "0                                 0                                    1   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   domain_match_references_to  domain_match_from_reply-to  \\\n",
       "0                           1                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           1   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   domain_match_to_from  domain_match_to_message-id  domain_match_to_received  \\\n",
       "0                     0                           0                         0   \n",
       "1                     0                           0                         0   \n",
       "2                     0                           0                         1   \n",
       "3                     0                           0                         0   \n",
       "4                     0                           0                         0   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XR4Wc3Fqh_W"
   },
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__criterion': 'entropy', 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 150} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('rf',\n",
      "                 RandomForestClassifier(criterion='entropy',\n",
      "                                        n_estimators=150))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 75.91836734693878\n",
      "F1 Score: 80.5921052631579\n",
      "Recall: 100.0\n",
      "Precision: 67.4931129476584\n",
      "ROC AUC: 75.91836734693878\n",
      "Confusion Matrix: [[127 118]\n",
      " [  0 245]]\n",
      "CPU times: user 425 ms, sys: 57.6 ms, total: 482 ms\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"rf\", RandomForestClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'rf__n_estimators': [100, 150],\n",
    "                  'rf__criterion': ['entropy', 'gini'],\n",
    "                  'rf__min_samples_split': [2, 3],\n",
    "                  'rf__min_samples_leaf': [1, 2],\n",
    "                  'rf__max_features': ['sqrt', 'log2']} # removed 'auto' \n",
    "         \n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "rf_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "original_predict = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, original_predict)*100)\n",
    "print('F1 Score:', f1_score(y_test, original_predict)*100)\n",
    "print('Recall:', recall_score(y_test, original_predict)*100)\n",
    "print('Precision:', precision_score(y_test, original_predict)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, original_predict)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, original_predict))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty: Trying hyperparameter tuning with RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'rf__n_estimators': 100, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 1, 'rf__max_features': 'sqrt', 'rf__criterion': 'entropy'}\n",
      "Best score is: 0.9859223300970873\n",
      "Best model is: Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('rf', RandomForestClassifier(criterion='entropy'))])\n",
      "\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 75.91836734693878\n",
      "F1 Score: 80.5921052631579\n",
      "Recall: 100.0\n",
      "Precision: 67.4931129476584\n",
      "ROC AUC: 75.91836734693878\n",
      "Confusion Matrix: [[127 118]\n",
      " [  0 245]]\n",
      "CPU times: user 391 ms, sys: 56 ms, total: 447 ms\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter Tuning with RANDOM SEARCH\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"rf\", RandomForestClassifier())\n",
    "                ])\n",
    "\n",
    "rs_space={'rf__n_estimators': [100, 150],\n",
    "               'rf__criterion': ['entropy', 'gini'],\n",
    "               'rf__min_samples_split': [2, 3],\n",
    "               'rf__min_samples_leaf': [1, 2],\n",
    "               'rf__max_features': ['sqrt', 'log2']\n",
    "         }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_random = RandomizedSearchCV(pipe, rs_space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=10)\n",
    "model_random = rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_df = pd.DataFrame(model_random.cv_results_)\n",
    "print('Best hyperparameters are: '+str(model_random.best_params_))\n",
    "print('Best score is: '+str(model_random.best_score_))\n",
    "print('Best model is: '+str(model_random.best_estimator_))\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = model_random.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "original_predict = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('\\nORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, original_predict)*100)\n",
    "print('F1 Score:', f1_score(y_test, original_predict)*100)\n",
    "print('Recall:', recall_score(y_test, original_predict)*100)\n",
    "print('Precision:', precision_score(y_test, original_predict)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, original_predict)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, original_predict))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#rf_df[rf_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBX1OIt-s3J8"
   },
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp__activation': 'tanh', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (20,), 'mlp__learning_rate': 'constant', 'mlp__solver': 'adam'} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('mlp',\n",
      "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(20,),\n",
      "                               max_iter=500))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 80.40816326530611\n",
      "F1 Score: 76.11940298507463\n",
      "Recall: 62.44897959183674\n",
      "Precision: 97.45222929936305\n",
      "ROC AUC: 80.40816326530613\n",
      "Confusion Matrix: [[241   4]\n",
      " [ 92 153]]\n",
      "CPU times: user 2 s, sys: 350 ms, total: 2.35 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"mlp\", MLPClassifier(max_iter=500))\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'mlp__hidden_layer_sizes': [(20,), (20,20), (40,), (40,40)],\n",
    "                   'mlp__activation': ['tanh', 'relu'],\n",
    "                   'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "                   'mlp__solver': ['adam', 'sgd'],\n",
    "                   'mlp__alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "mlp_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#mlp_df[mlp_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty: Trying hyperparameter tuning with RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: {'mlp__solver': 'adam', 'mlp__learning_rate': 'adaptive', 'mlp__hidden_layer_sizes': (40, 40), 'mlp__alpha': 0.001, 'mlp__activation': 'relu'}\n",
      "Best score is: 0.9859192585331075\n",
      "Best model is: Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('mlp',\n",
      "                 MLPClassifier(alpha=0.001, hidden_layer_sizes=(40, 40),\n",
      "                               learning_rate='adaptive', max_iter=500))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 94.89795918367348\n",
      "F1 Score: 94.88752556237218\n",
      "Recall: 94.6938775510204\n",
      "Precision: 95.08196721311475\n",
      "ROC AUC: 94.89795918367346\n",
      "Confusion Matrix: [[233  12]\n",
      " [ 13 232]]\n",
      "CPU times: user 6.27 s, sys: 510 ms, total: 6.78 s\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter Tuning with RANDOM SEARCH\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"mlp\", MLPClassifier(max_iter=500))\n",
    "                ])\n",
    "\n",
    "rs_space={'mlp__hidden_layer_sizes': [(20,), (20,20), (40,), (40,40)],\n",
    "            'mlp__activation': ['tanh', 'relu'],\n",
    "            'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "            'mlp__solver': ['adam', 'sgd'],\n",
    "            'mlp__alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "mlp_random = RandomizedSearchCV(pipe, rs_space, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "model_random = mlp_random.fit(X_train, y_train)\n",
    "\n",
    "mlp_df = pd.DataFrame(model_random.cv_results_)\n",
    "print('Best hyperparameters are: '+str(model_random.best_params_))\n",
    "print('Best score is: '+str(model_random.best_score_))\n",
    "print('Best model is: '+str(model_random.best_estimator_))\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = model_random.best_estimator_\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#mlp_df[mlp_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 1, 'lr__fit_intercept': True, 'lr__penalty': 'l1', 'lr__solver': 'saga', 'lr__tol': 0.0001} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('lr',\n",
      "                 LogisticRegression(C=1, max_iter=2000, penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 98.83720930232558\n",
      "F1 Score: 98.9010989010989\n",
      "Recall: 98.9010989010989\n",
      "Precision: 98.9010989010989\n",
      "ROC AUC: 98.83326549993217\n",
      "Confusion Matrix: [[240   3]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 91.42857142857143\n",
      "F1 Score: 90.78947368421053\n",
      "Recall: 84.48979591836735\n",
      "Precision: 98.10426540284361\n",
      "ROC AUC: 91.42857142857143\n",
      "Confusion Matrix: [[241   4]\n",
      " [ 38 207]]\n",
      "CPU times: user 1.72 s, sys: 985 ms, total: 2.7 s\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Original: Hyperparameter Tuning with GRID SEARCH\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"lr\", LogisticRegression(max_iter=2000))\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'lr__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                  'lr__fit_intercept': [True, False],\n",
    "                  'lr__tol': [0.0001, 0.001],\n",
    "                  'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                  'lr__C': [0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "lr_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#lr_df[lr_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 1, 'svc__degree': 3, 'svc__kernel': 'rbf', 'svc__tol': 0.001} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()), ('svc', SVC(C=1))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 80.20408163265306\n",
      "F1 Score: 82.77087033747779\n",
      "Recall: 95.10204081632652\n",
      "Precision: 73.27044025157232\n",
      "ROC AUC: 80.20408163265307\n",
      "Confusion Matrix: [[160  85]\n",
      " [ 12 233]]\n",
      "CPU times: user 1.43 s, sys: 479 ms, total: 1.91 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"svc\", SVC())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'svc__C': [0.1, 1, 10],\n",
    "                  'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  'svc__degree': [3, 4, 5],\n",
    "                  'svc__tol': [0.001, 0.0001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "svm_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#svm_df[svm_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__ccp_alpha': 0, 'dt__criterion': 'entropy', 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 2} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('dt',\n",
      "                 DecisionTreeClassifier(ccp_alpha=0, criterion='entropy'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 75.91836734693878\n",
      "F1 Score: 80.5921052631579\n",
      "Recall: 100.0\n",
      "Precision: 67.4931129476584\n",
      "ROC AUC: 75.91836734693878\n",
      "Confusion Matrix: [[127 118]\n",
      " [  0 245]]\n",
      "CPU times: user 353 ms, sys: 50.5 ms, total: 403 ms\n",
      "Wall time: 564 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"dt\", DecisionTreeClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'dt__criterion': ['entropy', 'gini'],\n",
    "                  'dt__min_samples_split': [2, 3, 4],\n",
    "                  'dt__min_samples_leaf': [1, 2, 3],\n",
    "                  'dt__ccp_alpha': [0, 0.005, 0.01, 0.025, 0.05, 0.1]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "dt_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#dt_df[dt_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes (Gaussian)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gnb__var_smoothing': 1e-09} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()), ('gnb', GaussianNB())])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 89.72868217054264\n",
      "F1 Score: 91.00169779286928\n",
      "Recall: 98.16849816849816\n",
      "Precision: 84.81012658227847\n",
      "ROC AUC: 89.20770587437255\n",
      "Confusion Matrix: [[195  48]\n",
      " [  5 268]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 50.0\n",
      "F1 Score: 0.0\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "ROC AUC: 50.0\n",
      "Confusion Matrix: [[245   0]\n",
      " [245   0]]\n",
      "CPU times: user 28.8 ms, sys: 5.6 ms, total: 34.4 ms\n",
      "Wall time: 52.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"gnb\", GaussianNB())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'gnb__var_smoothing': [1E-9, 1E-10, 1E-8]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "nb_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#nb_df[nb_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab__algorithm': 'SAMME', 'ab__learning_rate': 1.75, 'ab__n_estimators': 150} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('ab',\n",
      "                 AdaBoostClassifier(algorithm='SAMME', learning_rate=1.75,\n",
      "                                    n_estimators=150))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 98.83720930232558\n",
      "F1 Score: 98.9010989010989\n",
      "Recall: 98.9010989010989\n",
      "Precision: 98.9010989010989\n",
      "ROC AUC: 98.83326549993217\n",
      "Confusion Matrix: [[240   3]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 81.22448979591836\n",
      "F1 Score: 84.19243986254295\n",
      "Recall: 100.0\n",
      "Precision: 72.70029673590504\n",
      "ROC AUC: 81.22448979591836\n",
      "Confusion Matrix: [[153  92]\n",
      " [  0 245]]\n",
      "CPU times: user 742 ms, sys: 105 ms, total: 847 ms\n",
      "Wall time: 8.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"ab\", AdaBoostClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'ab__n_estimators': [50, 100, 150, 200],\n",
    "                  'ab__learning_rate': [0.95, 1, 1.05, 1.25, 1.5, 1.75, 2],\n",
    "                  'ab__algorithm': ['SAMME', 'SAMME.R']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "ab_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#ab_df[ab_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GradientBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__learning_rate': 0.1, 'gbc__max_features': 'sqrt', 'gbc__n_estimators': 200} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('gbc',\n",
      "                 GradientBoostingClassifier(max_features='sqrt',\n",
      "                                            n_estimators=200))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 75.91836734693878\n",
      "F1 Score: 80.5921052631579\n",
      "Recall: 100.0\n",
      "Precision: 67.4931129476584\n",
      "ROC AUC: 75.91836734693878\n",
      "Confusion Matrix: [[127 118]\n",
      " [  0 245]]\n",
      "CPU times: user 593 ms, sys: 76 ms, total: 669 ms\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"gbc\", GradientBoostingClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'gbc__max_features': ['sqrt', 'log2'], # Removed 'auto'\n",
    "                   'gbc__learning_rate': [0.05, 0.1, 0.2, 0.25, 0.30, 0.35, 0.40, 0.5, 0.6, 0.7, 0.9],\n",
    "                   'gbc__n_estimators': [100, 200]\n",
    "                  }\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "gb_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#gb_df[gb_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__algorithm': 'auto', 'knn__leaf_size': 15, 'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'} \n",
      "\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(leaf_size=15, n_neighbors=10, p=1,\n",
      "                                      weights='distance'))])\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 76.93877551020408\n",
      "F1 Score: 80.48359240069084\n",
      "Recall: 95.10204081632652\n",
      "Precision: 69.76047904191617\n",
      "ROC AUC: 76.93877551020407\n",
      "Confusion Matrix: [[144 101]\n",
      " [ 12 233]]\n",
      "CPU times: user 1.13 s, sys: 616 ms, total: 1.75 s\n",
      "Wall time: 576 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe = Pipeline([(\"scale\", StandardScaler()),\n",
    "                (\"knn\", KNeighborsClassifier())\n",
    "                ])\n",
    "\n",
    "param_grid_list = {'knn__n_neighbors': [1, 10, 20],\n",
    "                  'knn__weights': ['uniform', 'distance'],\n",
    "                  'knn__p': [1, 2],\n",
    "                  'knn__algorithm': ['auto'],\n",
    "                  'knn__leaf_size': [15, 30, 45]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid_list, cv=10, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Find the best hyperparameters (using 10 fold CV with the hold out fold being the validation set)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Check the hyperparameter results\n",
    "knn_df = pd.DataFrame(grid.cv_results_)\n",
    "print(grid.best_params_, '\\n')\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Get the best performing model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the best performing model on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "### Validation\n",
    "val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Get the evaluation metrics\n",
    "print('ORIGINAL EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "print('Recall:', recall_score(y_test, predictions)*100)\n",
    "print('Precision:', precision_score(y_test, predictions)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the validation evaluation metrics\n",
    "print('\\nVALIDATION EVALUATION METRICS')\n",
    "print('-----------------------------')\n",
    "print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "#knn_df[knn_df['rank_test_score'] <= 5].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacked Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 82.24489795918367\n",
      "F1 Score: 84.26763110307414\n",
      "Recall: 95.10204081632652\n",
      "Precision: 75.64935064935064\n",
      "ROC AUC: 82.24489795918367\n",
      "Confusion Matrix: [[170  75]\n",
      " [ 12 233]]\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 83.46938775510205\n",
      "F1 Score: 85.19195612431444\n",
      "Recall: 95.10204081632652\n",
      "Precision: 77.15231788079471\n",
      "ROC AUC: 83.46938775510203\n",
      "Confusion Matrix: [[176  69]\n",
      " [ 12 233]]\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 81.83673469387756\n",
      "F1 Score: 83.96396396396396\n",
      "Recall: 95.10204081632652\n",
      "Precision: 75.16129032258064\n",
      "ROC AUC: 81.83673469387755\n",
      "Confusion Matrix: [[168  77]\n",
      " [ 12 233]]\n",
      "ORIGINAL EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 99.03100775193798\n",
      "F1 Score: 99.08256880733946\n",
      "Recall: 98.9010989010989\n",
      "Precision: 99.26470588235294\n",
      "ROC AUC: 99.0390268168046\n",
      "Confusion Matrix: [[241   2]\n",
      " [  3 270]]\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "VALIDATION EVALUATION METRICS\n",
      "-----------------------------\n",
      "Accuracy: 92.44897959183673\n",
      "F1 Score: 92.6441351888668\n",
      "Recall: 95.10204081632652\n",
      "Precision: 90.31007751937985\n",
      "ROC AUC: 92.44897959183672\n",
      "Confusion Matrix: [[220  25]\n",
      " [ 12 233]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "'''\n",
    "base_learners = [('rf', RandomForestClassifier(criterion='entropy', max_features='auto', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "'''\n",
    "\n",
    "base_learners_set1 = [('rf', RandomForestClassifier(criterion='entropy', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance'))]\n",
    "\n",
    "base_learners_set2 = [('rf', RandomForestClassifier(criterion='entropy', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')), \n",
    "                ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners_set3 = [('rf', RandomForestClassifier(criterion='entropy',  min_samples_leaf=1, min_samples_split=3, n_estimators=100)),\n",
    "                ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners_set4 = [('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners = []\n",
    "base_learners.append(base_learners_set1)\n",
    "base_learners.append(base_learners_set2)\n",
    "base_learners.append(base_learners_set3)\n",
    "base_learners.append(base_learners_set4)\n",
    "\n",
    "for base_learner_group in base_learners:\n",
    "\n",
    "    meta_learner = LogisticRegression()\n",
    "\n",
    "    clf = StackingClassifier(estimators=base_learner_group, final_estimator=meta_learner)\n",
    "\n",
    "    # Train the stacked model on the full training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    ### Validation\n",
    "    val_pred = clf.predict(X_val)\n",
    "\n",
    "    # Get the evaluation metrics\n",
    "    print('ORIGINAL EVALUATION METRICS')\n",
    "    print('-----------------------------')\n",
    "    print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "    print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "    print('Recall:', recall_score(y_test, predictions)*100)\n",
    "    print('Precision:', precision_score(y_test, predictions)*100)\n",
    "    print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "    print('-----------------------------------------\\n')\n",
    "\n",
    "    # Get the validation evaluation metrics\n",
    "    print('\\nVALIDATION EVALUATION METRICS')\n",
    "    print('-----------------------------')\n",
    "    print('Accuracy:', accuracy_score(y_val, val_pred)*100)\n",
    "    print('F1 Score:', f1_score(y_val, val_pred)*100)\n",
    "    print('Recall:', recall_score(y_val, val_pred)*100)\n",
    "    print('Precision:', precision_score(y_val, val_pred)*100)\n",
    "    print('ROC AUC:', roc_auc_score(y_val, val_pred)*100)\n",
    "    print('Confusion Matrix:', confusion_matrix(y_val, val_pred))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sCMEqns8jY-1",
    "F5A997Ekjh0E",
    "KvWjb8GwOM-y",
    "rJJ4WHSBFTWc",
    "nqHSyA4SyJ7K",
    "xhkNe2ql-cFP",
    "fMwsPWxpi2cG",
    "-_nEjwFOjUDI",
    "MyekmTK3q5TC",
    "RkQzXPCXUxme",
    "P1zXPP6trxSN",
    "iGZ561te2ONO",
    "rffvHbuzGO8c"
   ],
   "name": "Capstone Project - Email Headers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
